{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Introduction to Model Selection Methods </center>\n",
    "<br><br>\n",
    "<center> Zhangyi Hu </center>\n",
    "<center> Oct. 16, 2016</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "cd6f0a2a-2288-4cf2-a060-7f3e818a3a66"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center> Contents </center>\n",
    "\n",
    "- Review on basic terminology of statistical learning \n",
    "- Motivation of model selection\n",
    "- Three approaches of model selection\n",
    "    - Estimate test error\n",
    "    - Estimate information loss\n",
    "    - Estimate the posterior probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Basic terminology of supervised statistical learning </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "###  Data = feature + response "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Model  predicts response based on feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Data can be *Training* set or *Test* set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Data = feature + response </center>\n",
    "Other names for *feature*:\n",
    "- regressor, predictor\n",
    "- independent(input, explanatory) variable\n",
    "- $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Other names for *response*:\n",
    "- regressand\n",
    "- dependent(output, explained) variable\n",
    "- $Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Model  predicts response based on feature </center>\n",
    "- Model is mathematically defined by a set of parameters $\\{\\theta_i\\}$\n",
    "  - Ture model: $\\mathbf{Y}=f_{\\theta}\\left(\\mathbf{X}\\right)+\\mathbf{\\varepsilon}$\n",
    "  - Trained model:\n",
    "$\\hat{\\mathbf{Y}}=g_{\\hat{\\theta}}\\left(\\mathbf{X}\\right)$\n",
    "  - We usually assume independence between $\\mathbf{\\varepsilon}$ and $\\mathbf{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The process of finding the value of parameters is called *Model Trainning*: \n",
    "$$\\{\\theta_{i}\\}\\rightarrow\\{\\hat{\\theta}_{i}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The process of choosing the subset of parameter space is called *Model Selection*: \n",
    "$$\\{\\hat{\\theta}_{i}\\}_{i=1}^{p}\\mbox{ or }\\{\\hat{\\theta}_{i}\\}_{i=1}^{p+q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Training set and Test set </center>\n",
    "- Both are data, with feature and response\n",
    "- Training set is used to train the model, i.e. obtain $\\{\\theta_i\\}$\n",
    "  - The difference between model prediction and response in the training set: **Training Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Test set are used to evaluate the model\n",
    "  - Test set are usually not available to the model designer\n",
    "  - e.g. The the data provided by the end user of the trained model\n",
    "  - The difference between prediction and response in the test set: **Test Error**\n",
    "    - When the test set has the same features as training set but new observations of response: **In-sample Test Error**\n",
    "    - When the test set has different feature: **Extra-sample Test Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Usually, the model designer randomly put away part of available data and pretend he or she doesn't know it and, at the final stage, use it as test set\n",
    "- Generally, reducing *Test Error* is the main effort of statistical learning engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Motivation of model selection </center>\n",
    "### Model, by definition, need to be relatively simple to be practical\n",
    "### With no constraint on parameter space, a model can fit anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>One extreme example</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "ax.set_xlabel('feature')\n",
    "ax.set_ylabel('response')\n",
    "ax.set_xlim([0.8, 6.0])\n",
    "ax.set_ylim([1.0, 9.0])\n",
    "x = [1,2,3,4]\n",
    "y = [2.1, 3.9, 6.0, 7.8]\n",
    "ax.plot(x, y, 'o', ms=8)\n",
    "for xy in zip(x, y):\n",
    "    ax.annotate(' (%s, %s)' % xy, xy=xy, textcoords='data') \n",
    "fig.savefig('resource/FourPoints.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='resource/FourPoints.png' width='70%'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center> A type of model that can fit any available data </center>\n",
    "<center> Its training error is zero! </center>\n",
    "<center> $y=\\begin{cases}\n",
    "2.1 & x=1\\\\\n",
    "3.9 & x=2\\\\\n",
    "6.0 & x=3\\\\\n",
    "7.8 & x=4\\\\\n",
    "x & \\mbox{otherwise}\n",
    "\\end{cases}$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> Do we have a better model? </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Your brain just drew a straight line subconsciously \n",
    "#### It is the nature of intelligence to favor one simple line compared with 4 points\n",
    "#### Artificial Intelligence behaviors in a similar way\n",
    "#### The purpose of model selection is find a proper constraint on the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Examples of model parameter Constraints </center>\n",
    "Let the parameters be a $p$-vector $\\mathbf{\\beta} = [\\beta_1,\\dots,\\beta_p]^T$\n",
    "\n",
    "- $\\left\\Vert \\mathbf{\\beta}\\right\\Vert _{0} \\le n$, subset selection\n",
    "- $\\left\\Vert \\mathbf{\\beta}\\right\\Vert _{1} \\le a$, LASSO\n",
    "- $\\left\\Vert \\mathbf{\\beta}\\right\\Vert _{2} \\le a$, Shrinkage\n",
    "\n",
    "Model selection find the suitable $n$ or $\\lambda$, which defines a subset of the parameter space\n",
    "\n",
    "For a continuous parameter such as $a$, model selection is also called model tunning\n",
    "\n",
    "The Lagrange form of those constraints are called regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Three approaches of model selection</center>\n",
    "#### Estimate expected test error (Mallow's $C_p$, Cross validation, Bootstrap)\n",
    "#### Estimate information loss (Akaike Information Criterion)\n",
    "#### Estimate the posterior probability (Bayesian Information Criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center>Test error function</center>\n",
    "- Test error(or loss) function measures how bad a prediction is with test data:\n",
    "\n",
    "<center>e.g. squared-error loss: $L(y,\\hat{y})=\\left(y-\\hat{y}\\right)^{2}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### <center>Decomposition of expected test error</center>\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{Err}\\left(x_{0}\\right) & =\\mathbb{E}\\left[(Y-\\hat{f}(x_{0}))^{2}\\right]\\\\\n",
    " & =\\mathbb{E}\\left[\\left(f(x_{0})+\\varepsilon-\\hat{f}(x_{0})\\right)^{2}\\right]\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}+\\mathbb{E}\\left[\\left(f(x_{0})-\\mathbb{E}\\left[\\hat{f}(x_{0})\\right]+\\mathbb{E}\\left[\\hat{f}(x_{0})\\right]-\\hat{f}(x_{0})\\right)^{2}\\right]\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}+\\mathbb{E}\\left[\\left(f(x_{0})-\\mathbb{E}\\left[\\hat{f}(x_{0})\\right]\\right)^{2}\\right]+\\mathbb{E}\\left[\\left(\\mathbb{E}\\left[\\hat{f}(x_{0})\\right]-\\hat{f}(x_{0})\\right)^{2}\\right]\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}+\\mbox{Bias}^{2}\\left(f(x_{0})\\right)+\\mbox{Var}\\left(\\hat{f}(x_{0})\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Bias-Variance trade off </center>\n",
    "- With model getting more complex, the bias usually decreases\n",
    "- However, at the same time the variance usually goes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- e.g. in OLS\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mbox{Var}\\left(\\hat{f}(x_{i})\\right) & = & \\frac{p}{N}\\sigma_{\\varepsilon}^{2}\\\\\n",
    "\\mbox{Var}\\left(\\hat{f}(x_{0})\\right) & \\sim & \\frac{p}{N}\\sigma_{\\varepsilon}^{2},\\quad N\\rightarrow\\infty\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "$\\begin{array}{cc}\n",
    "x_{i} & x_{0}\\\\\n",
    "\\overline{\\mbox{in sample}} & \\overline{\\mbox{out of sample}}\n",
    "\\end{array}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Bias-Variance trade off </center>\n",
    "- Training error always decreases with increasingly complex model(or with more fitting effort)\n",
    "- It is a common mistake to use too complex a model with large prediction variance: **over fit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> Perhaps the most over fitted model </center>\n",
    "\n",
    "<center> $y=\\begin{cases}\n",
    "y_i & \\mathrm{if}\\quad x=x_i\\\\\n",
    "x & \\mbox{otherwise}\n",
    "\\end{cases}$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Bias-Variance trade off </center>\n",
    "<center><img src='resource/ESLII_Fig7.2.png' width='60%'/></center>\n",
    "<center><font size=\"3\">T. Hastie, R. Tibshirani, and J. Friedman, Elements of statistical learning, Figure 7.2</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Estimate expected test error</center>\n",
    "- Select the subset of parameter space whose best fitted model produces the smallest expected test error\n",
    "\n",
    "- With many loss functions, the difference between expected in sample test error and expected training error plus some term can be caculated explicitly\n",
    "$$\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}\\right]=\\mathbb{E}_{\\mathbf{y}}\\left[\\overline{\\mbox{err}}\\right]+\\frac{2}{N}\\sum_{i=1}^{N}\\mathrm{Cov}\\left(\\hat{y}_{i},y_{i}\\right)$$\n",
    "\n",
    "- <a href='http://nbviewer.jupyter.org/github/hzzyyy/Presentations/blob/master/Model%20Selection/proofs/In-sample%20test%20error%20and%20training%20error.ipynb'>Proof for squared error loss function </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Mallow's $C_p$ </center>\n",
    "- Mallow's $C_p$ estimates expected in-sample test error for OLS\n",
    "- Let's denote the projection matrix of OLS as:\n",
    "$$\\mathbf{S}=\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}$$\n",
    "- Then we can prove that:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{N}\\mbox{Cov}\\left(\\hat{y}_{i},y_{i}\\right) & =\\sigma_{\\varepsilon}^{2}\\mbox{Tr}\\left(\\mathbf{S}\\right)=\\sigma_{\\varepsilon}^{2}p\n",
    "\\end{align*}\n",
    "$$\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}\\right]=\\mathbb{E}_{\\mathbf{y}}\\left[\\overline{\\mbox{err}}\\right]+\\frac{2p}{N}\\sigma_{\\varepsilon}^{2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Mallow's $C_p$ </center>\n",
    "- Mallow's $C_p$ estimates in-sample test error for OLS\n",
    "- $\\overline{\\mbox{err}}=\\frac{\\mbox{RSS}}N$ estimates\n",
    "$\\mathbb{E}_{\\mathbf{y}}\\left[\\overline{\\mbox{err}}\\right]$\n",
    "(moment estimator with one sample)\n",
    "- $\\sigma_{\\varepsilon}^2$ is estimated from another low bias model, which is most likely overfitted\n",
    "- In subset selection\n",
    "$C_{p}=\\frac{\\mbox{RSS}_{p}}{N}+\\frac{2p}{N}\\frac{\\mbox{RSS}_{K}}{N-K}$\n",
    "- In OLS, extra-sample test error approaches in-sample test error \n",
    "as $N$ gets large (<a href='http://nbviewer.jupyter.org/github/hzzyyy/Presentations/blob/master/Model%20Selection/proofs/In-sample%20test%20error%20and%20extra-sample%20test%20error.ipynb'>proof</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Estimate expected test error</center>\n",
    "- We can also directly estimate extra-sample test error, by using part of the training set as \"test set\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It is not the real test, because test set is used at the final stage, after the model selection is completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It is not used in model training with given parameter space constraint, so it is not training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It is in between training set and test set and we name it **validation set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Estimate expected test error</center>\n",
    "| <font size='5'>Training set </font>|<font size='5'> Validation set </font>|\n",
    "| :-----------------: | :------------: |\n",
    "| $\\mathcal{T}$ | $\\mathcal{V} = \\{(x^j, y^j)\\}_{j=1}^{M} $ |\n",
    "- With one random partition between training and validation set, \n",
    "we can obtain one realization of the mean extra-sample test error\n",
    "$$\\frac{1}{M}\\sum_{j=1}^{M}L(y^{j},\\hat{y}^{j})$$\n",
    "- This is one realization of the moment estimator of the expected extra-sample test error\n",
    "$$\\mbox{Err}_{\\mathcal{T}}=\\mathbb{E}_{y^j}\\left[L(y^{j},\\hat{y}^{j})\\vert\\mathcal{T}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- If we use only one such realization to estimate, the variance is too large. \n",
    "By central limit theorem, we can use the mean of $K$ such realizations to reduce the variance by a factor of $1/K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| <font size='5'>Training sets </font>|<font size='5'> Validation sets </font>|\n",
    "| :-----------------: | :------------: |\n",
    "| $\\mathcal{T}_1$ | $\\mathcal{V}_1 = \\{(x_1^j, y_1^j)\\}_{j=1}^{M} $ |\n",
    "| $\\cdots$ | $\\cdots$ |\n",
    "| $\\mathcal{T}_i$ | $\\mathcal{V}_i = \\{(x_i^j, y_i^j)\\}_{j=1}^{M} $ |\n",
    "| $\\cdots$ | $\\cdots$ |\n",
    "| $\\mathcal{T}_K$ | $\\mathcal{V}_K = \\{(x_K^j, y_K^j)\\}_{j=1}^{M} $ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The mean of the $K$ means of the extra-sample test error is\n",
    "$$\\frac{1}{K}\\sum_{i=1}^{K}\\frac{1}{M}\\sum_{j=1}^{M}L(y_{i}^{j},\\hat{y}_{i}^{j})$$\n",
    "- Which is a moment estimator of the iterated expected extra-sample test error\n",
    "$$\\mbox{Err}=\\mathbb{E}_{\\mathcal{T}}\\left[\\mbox{Err}_{\\mathcal{T}}\\right]=\\mathbb{E}_{\\mathcal{T}}\\left[\\mathbb{E}_{y^{j}}\\left[L(y^{j},\\hat{y}^{j})\\vert\\mathcal{T}\\right]\\right]$$\n",
    "- There are two sources of randomness\n",
    " - realization of the random test response $y_i^j$, conditional on $\\mathcal{T}_i$\n",
    " - realization of the random partition between $\\mathcal{T}_i$ and $\\mathcal{V}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Cross validation </center> \n",
    "- In $K$-fold cross validation, the original training set $\\mathcal{T}$ is evenly divided to $K$ subsets, each with size $\\frac{N}{K}$ \n",
    "- Each time use one of them as the validation set and the rest $N -\\frac{N}{K}$ points as the training set\n",
    "- The validation sets are mutually disjoint, this is why it is called cross validation\n",
    "<center><img src='resource/5-CV.png' width='80%'></img></center>\n",
    "<center><font size=\"3\">5-fold cross validation, T. Hastie, R. Tibshirani, and J. Friedman, Elements of statistical learning</font></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Bootstrap</center>\n",
    "- In Bootstrap, the validation set is the original training set $\\mathcal{T}$ \n",
    "- The bootstrap training set $\\mathcal{T}_i$ is sampled from the original training set with replacement, each with size $N$\n",
    "- Use training data sets generated from the original training set by random sampling, this is why it is called bootstrap\n",
    "- With $B$ bootstrap sets, one naive estimation for Err is\n",
    "$$\\frac{1}{B}\\sum_{b=1}^{B}\\frac{1}{N}\\sum_{i=1}^{N}L\\left(y_{i},f^{b}\\left(x_{i}\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Bootstrap</center>\n",
    "- However, a bootstrap set overlaps with the validation set, \n",
    "some of the $L\\left(y_{i},f^{b}\\left(x_{i}\\right)\\right)$ are training error not test error\n",
    "- This will lead to an underestimate of test error\n",
    "- Excluding those overlapping points in the validation set gets another estimate $\\widehat{\\mbox{Err}}$\n",
    "- The actual estimate of extra-sample test error is usually a \n",
    "weighted average of $\\widehat{\\mbox{Err}}$ and the training error \n",
    "$\\overline{\\mbox{err}}$\n",
    "$$\\omega\\cdot\\overline{\\mbox{err}}+(1-\\omega)\\widehat{\\mbox{Err}}$$\n",
    "- One popular choice of such $\\omega$ for light-fitting cases is $e^{-1}$\n",
    "- When the over-fit is too strong, we need to further modify $\\omega$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Summary for test error based criteria</center>\n",
    "- Mallow's $C_p$ indirectly estimates the expected extra-sample test\n",
    "error \n",
    "$\\mathbb{E}_{\\mathbf{y}}\\left[{\\mbox{Err}_{\\mathcal{T}}}\\right]$\n",
    " by an estimate of expected in-sample test error\n",
    " \n",
    "\\begin{eqnarray*}\n",
    "C_{p} & \\sim & \\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}\\right]\\sim\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mathcal{T}}\\right]\\\\\n",
    " &  & \\mbox{as }N\\rightarrow\\infty\n",
    "\\end{eqnarray*}\n",
    "- Cross validation and bootstrap randomly choose training set and \"test\"(validation) set to estimate the expected extra-sample test error\n",
    "$$\\mathbb{E}_{\\mathcal{T}}\\left[\\mbox{Err}_{\\mathcal{T}}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Information Criterion</center>\n",
    "- Let's denote $\\mathbf{y} = [y_1,\\dots,y_N]^T$ as the realization of response of the training set $\\mathbf{Y}$,\n",
    "and $\\mathbf{\\Theta}= [\\theta_1,\\dots,\\theta_p]^T$ as the parameter vector\n",
    "- Suppose the true joint density of $\\mathbf{Y}$ is\n",
    "$$f\\left(\\mathbf{y}\\right)=\\prod_{i=1}^{N}f^{*}\\left(y_{i}\\right)$$\n",
    "- and the set of densities produced by model parameter vector as\n",
    "$$g_{\\Theta}\\left(\\mathbf{y}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Information Criterion</center>\n",
    "- Suppose we have several trained models $g_{\\hat{\\Theta_{j}}}\\left(\\mathbf{y}\\right)$, each from a subset of the parameter space\n",
    "- Information criterion tells us to pick the one with the least difference compared with $f\\left(\\mathbf{y}\\right)$ \n",
    "- The task is to quantify the difference between $f\\left(\\mathbf{y}\\right)$ and $g_{\\hat{\\Theta_{j}}}\\left(\\mathbf{y}\\right)$, and estimate it after a model is trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What properties do we want this quantity to have?\n",
    "    - what value should it take when $f(\\mathbf{Y})=g(\\mathbf{Y})$?\n",
    "    - what kind of values should it take when $f(\\mathbf{Y})\\ne g(\\mathbf{Y})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 0 for the first question and positive for the second question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Information Criterion</center>\n",
    "- This is positive definiteness, if it has symmetry and triangular inequality then it is a metric\n",
    "- It is tempting to just use a metric (aka distance function) because it is very geometrically intuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Symmetry is not necessary here, because we only pick one direction to study: from the model prediction density to the true density\n",
    "- Triangular inequality is not necessary here, because we don't need to describe the difference between two model prediction densities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Information Criterion</center>\n",
    "- Without the above two requirements, we have larger candidate pool, and Akaike picked good one, which is not a metric\n",
    "- Akaike's choice is Kullback–Leibler divergence, also called information loss, relative entropy etc.\n",
    "- It is sometimes informally called K-L distance, but we shouldn't because it is not a metric\n",
    "- It has a very simple formula and can be decomposed into two parts, one part depends only on the \"destination\" density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Kullback–Leibler divergence </center>\n",
    "- Suppose $f(x)$ and $g(x)$ are the density functions of two different random variables $X_2$ and $X_1$ from the same probability space $(\\Omega, \\Sigma, P)$ to the same state space (e.g. $\\mathbb{R}$)\n",
    "- The K-L divergence from $g$ to $f$ (or from $X_1$ to $X_2$) is the expected logarithm of the ratio of their densities, over the distribution $f$\n",
    "$$D_{KL}\\left(X_{1}\\Vert X_{2}\\right)=D_{KL}\\left(f\\Vert g\\right)=\\mathbb{E}_{f}\\left[\\ln\\frac{f(x)}{g(x)}\\right]\\\\$$\n",
    "- For random variable on $\\mathbb{R}$:\n",
    "$$ \\int_{-\\infty}^{+\\infty}\\ln\\frac{f(x)}{g(x)}\\cdot f(x)\\mbox{d}x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_ylim([-2.0, 2.0])\n",
    "ax.set_aspect('equal')\n",
    "x1 = np.linspace(-1.0,4.0, 3)\n",
    "y1 = x1-1.0\n",
    "x2 = np.linspace(0.15, 6.0, 200)\n",
    "y2 = np.log(x2)\n",
    "ax.plot(x1, y1, '-b', label='$x-1$')\n",
    "ax.plot(x2, y2, '-r', label='$\\ln(x)$')\n",
    "ax.plot([1.0], [0.0], '.g', ms=8)\n",
    "ax.legend(loc = 'best')\n",
    "fig.savefig('resource/Gibbs_Inequality.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Kullback–Leibler divergence </center>\n",
    "- The proof for positive definiteness is very simple\n",
    "<center><img src='resource/Gibbs_Inequality.png' width='100%'></img></center>\n",
    "<center>$\\ln(x) \\le x-1,\\, \\forall x>0$ with equality iff $x=1$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Kullback–Leibler divergence </center>\n",
    "- Gibbs inequality\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "D_{KL}\\left(f\\Vert g\\right)= & \\mathbb{E}_{f}\\left[\\ln\\frac{f(x)}{g(x)}\\right] & =-\\mathbb{E}_{f}\\left[\\ln\\frac{g(x)}{f(x)}\\right]\\\\\n",
    "\\ge & -\\mathbb{E}_{f}\\left[\\frac{g(x)}{f(x)}-1\\right] & =-\\mathbb{E}_{f}\\left[\\frac{g(x)}{f(x)}\\right]+1\\\\\n",
    "= & -\\mathbb{E}_{g}\\left[1\\right]+1\\\\\n",
    "= & 0\n",
    "\\end{eqnarray*}\n",
    "- With equality iff $\\,f(x)/g(x)=1$ almost everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Kullback–Leibler divergence </center>\n",
    "- K-L divergence can be decomposed into two parts\n",
    "$$D_{KL}\\left(f\\Vert g\\right)=\\mathbb{E}_{f}\\left[\\ln\\frac{f(x)}{g(x)}\\right]=\\mathbb{E}_{f}\\left[\\ln f(x)\\right]-\\mathbb{E}_{f}\\left[\\ln g(x)\\right]$$\n",
    "- The first term is, by definition, the negative of entropy, which is only related to the true distribution $f(x)$\n",
    "$$H\\left(f\\right)=-\\mathbb{E}_{f}\\left[\\ln f(x)\\right]$$\n",
    "- The second term is, by definition, the cross entropy from $g$ to $f$\n",
    "$$H\\left(f,g\\right)=-\\mathbb{E}_{f}\\left[\\ln g(x)\\right]$$\n",
    "- Picking the model with smaller K-L divergence, is equivalent to picking the model with smaller cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Akaike information criterion</center>\n",
    "- Let's go back to the original problem, what random variable(s) are we talking about regarding true distributions and model prediction distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The random variable(s) we care about is the response vector $\\mathbf{Y}$ in the training set, with the true density of \n",
    "$$f\\left(\\mathbf{y}\\right)$$\n",
    "- The model prediction density is\n",
    "$$g_{\\Theta}\\left(\\mathbf{y}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Akaike information criterion</center>\n",
    "- For each trained model $g_{\\hat{\\Theta}_j}\\left(\\mathbf{y}\\right)$, \n",
    "we want its cross entropy to $f$\n",
    "$$H\\left(f,g_{\\hat{\\Theta}_{j}}\\right)=-\\mathbb{E}_{f}\\left[\\ln g_{\\hat{\\Theta}_{j}}(\\mathbf{y})\\right]$$\n",
    "- So AIC tells use to pick the model with smallest cross entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>$\\fbox{Close, but not exact}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Akaike information criterion</center>\n",
    "- The key to understand Akaike's work is to see that the above cross entropy itself is random\n",
    "- The model parameter $\\hat{\\Theta}_{j}$ is a function of the realization of response vector $\\mathbf{y}'$ (e.g. OLS)\n",
    "- It is denoted as $\\mathbf{y}'$ in order to avoid conflict with the \n",
    "independent (or changing) variable of the density function\n",
    "$g_{\\hat{\\Theta}_j}\\left(\\mathbf{y}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Akaike information criterion</center>\n",
    "- Let's denote the prediction of $N$-vector of responses conditional on the \n",
    "training set as $\\left.\\hat{\\mathbf{Y}}\\right|\\mathbf{y}'$,\n",
    "with a density function as:\n",
    "$$\\left.\\hat{\\mathbf{Y}}\\right|\\mathbf{y}'\\sim g_{\\hat{\\Theta}_{j}\\left(\\mathbf{y}'\\right)}\\left(\\mathbf{y}\\right)$$\n",
    "- The response $N$-vector in the training set $\\mathbf{Y}$ has the true distribution of:\n",
    "$$\\mathbf{Y}\\sim f\\left(\\mathbf{y}\\right)$$\n",
    "- Let's denote the cross entropy more explicitly as a function of $\\mathbf{y}'$, \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "H\\left(\\mathbf{Y},\\left.\\hat{\\mathbf{Y}}\\right|\\mathbf{y}'\\right) & = & H\\left(f,g_{\\hat{\\Theta}_{j}\\left(\\mathbf{y}'\\right)}\\left(\\mathbf{y}\\right)\\right)\\\\\n",
    " & = & -\\mathbb{E}_{f}\\left[\\left.\\ln g_{\\hat{\\Theta}_{j}\\left(\\mathbf{y}'\\right)}(\\mathbf{y})\\right|\\mathbf{Y}=\\mathbf{y}'\\right]\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Akaike information criterion</center>\n",
    "- AIC tells us to pick the one with smallest expected cross entropy\n",
    "$$\\mathbb{E}_{f}\\left[H\\left(\\mathbf{Y},\\left.\\hat{\\mathbf{Y}}\\right|\\mathbf{y}'\\right)\\right]$$\n",
    "- The expectation is taken over $f$ because it is also the distribution of $\\mathbf{y}'$ \n",
    "- The expectation in cross entropy is also over $f$, to avoid conflict let's reformulate for better clarity:\n",
    "$$\\mathbb{E}_{\\mathbf{y}'}\\left[H\\left(\\mathbf{Y},\\left.\\hat{\\mathbf{Y}}\\right|\\mathbf{y}'\\right)\\right]=-\\mathbb{E}_{\\mathbf{y}'}\\mathbb{E}_{\\mathbf{y}|\\mathbf{y}'}\\left[\\left.\\ln g_{\\hat{\\Theta}_{j}\\left(\\mathbf{y}'\\right)}(\\mathbf{y})\\right|\\mathbf{Y}=\\mathbf{y}'\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Akaike information criterion</center>\n",
    "- How to estimate the expected cross entropy $\\mathbb{E}_{\\mathbf{y}'}\\left[H\\left(\\mathbf{Y},\\left.\\hat{\\mathbf{Y}}\\right|\\mathbf{y}'\\right)\\right]$?\n",
    "- Akaike found that it is closely related to the following quantity\n",
    "$$\\left.-\\ln g_{\\hat{\\Theta}_{j}\\left(\\mathbf{y}'\\right)}(\\mathbf{y})\\right|_{\\mathbf{y}=\\mathbf{y}'}$$\n",
    "- It is also the (negative log) likelihood function of \n",
    "parameter $\\Theta_{j}$ given the observation $\\mathbf{y}'$ of\n",
    "$\\mathbf{Y}$, evaluated at \n",
    "$\\hat{\\Theta}_{j}\\left(\\mathbf{y}'\\right)$\n",
    "$$-\\ln\\mathcal{L}\\left.\\left(\\left.\\Theta_{j}\\right|\\mathbf{y}'\\right)\\right|_{\\Theta_{j}=\\hat{\\Theta}_{j}\\left(\\mathbf{y}'\\right)}$$\n",
    "- If the model is trained by maximum likelihood estimation, then\n",
    "$$\\max_{\\Theta_{j}}\\left\\{ \\ln\\mathcal{L}\\left(\\left.\\Theta_{j}\\right|\\mathbf{y}'\\right)\\right\\} =\\ln\\mathcal{L}\\left.\\left(\\left.\\Theta_{j}\\right|\\mathbf{y}'\\right)\\right|_{\\Theta_{j}=\\hat{\\Theta}_{j}\\left(\\mathbf{y}'\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Akaike information criterion</center>\n",
    "- Akaike (1973, 1974) proved the following\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "-\\max_{\\Theta_{j}}\\left\\{ \\ln\\mathcal{L}\\left(\\left.\\Theta_{j}\\right|\\mathbf{y}'\\right)\\right\\} +p & \\sim & \\mathbb{E}_{\\mathbf{y}'}\\left[H\\left(\\mathbf{Y},\\left.\\hat{\\mathbf{Y}}\\right|\\mathbf{y}'\\right)\\right]\\\\\n",
    "\\mbox{as }N & \\rightarrow & \\infty\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- MLE gives you the first term for free. With maximum likelihood estimator $\\hat{\\Theta}_{j}$ based on \n",
    "training response $\\mathbf{y}'$, the AIC is:\n",
    "$$-2\\ln\\mathcal{L}\\left(\\left.\\hat{\\Theta}_{j}\\right|\\mathbf{y}'\\right)+2p$$\n",
    "\n",
    "- For linear model with Gauss error\n",
    "$$-2\\ln\\mathcal{L}\\left(\\left.\\hat{\\Theta}_{j}\\right|\\mathbf{y}'\\right)=N\\ln\\left(\\hat{\\sigma}^{2}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Corrected Akaike information criterion</center>\n",
    "- What if $N$ is not large enough? In that case, the AIC is no longer a good estimator of expected cross entropy\n",
    "- When $N$ is small (e.g. $\\frac{N}{p_{\\mathrm{max}}} < 40$), there is a correction to make this estimate good again:\n",
    "$$\\mbox{AIC}_{\\mbox{c}}=-2\\ln\\mathcal{L}\\left(\\left.\\hat{\\Theta}_{j}\\right|\\mathbf{y}'\\right)+p+\\frac{2p(p+1)}{N-p-1}$$\n",
    "- It is obvious that $\\mbox{AIC}_c$ converges to AIC when $N$ gets \n",
    "large, so always use $\\mbox{AIC}_c$ instead of AIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Summary for information criterion</center>\n",
    "- An information criterion selects the model with least expected information loss (aka K-L divergence) (from the prediction model density to the true density)\n",
    "- This is equivalent to select the model with least cross entropy\n",
    "- AIC and $\\mbox{AIC}_c$ are estimates of such expected cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Bayesian information criterion</center>\n",
    "- Developed by Schwarz (1978), I prefer to call it Schwarz criterion, or simply Bayesian criterion\n",
    "- Because it is not an information criterion\n",
    "- Under Bayesian philosophy, the candidate models \n",
    "$\\{\\mathcal{M}_{j}\\}_{j=1}^M$ have a posterior probability distribution\n",
    "$$P\\left(\\left.\\mathcal{M}_{j}\\right|\\mathbf{y}\\right)\\propto P\\left(\\mathcal{M}_{j}\\right)p\\left(\\left.\\mathbf{y}\\right|\\mathcal{M}_{j}\\right)$$\n",
    "- This criterion tries to estimate $P\\left(\\left.\\mathcal{M}_{j}\\right|\\mathbf{y}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Bayesian information criterion</center>\n",
    "- We use the most uninformative (maximum entropy) prior distribution\n",
    "$$P\\left(\\mathcal{M}_{j}\\right)=\\frac{1}{M}$$\n",
    "- As a result,\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P\\left(\\left.\\mathcal{M}_{j}\\right|\\mathbf{y}\\right) & \\propto & p\\left(\\left.\\mathbf{y}\\right|\\mathcal{M}_{j}\\right)\\\\\n",
    "\\ln P\\left(\\left.\\mathcal{M}_{j}\\right|\\mathbf{y}\\right) & = & \\ln p\\left(\\left.\\mathbf{y}\\right|\\mathcal{M}_{j}\\right)+C\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Bayesian information criterion</center>\n",
    "- Each Model $\\mathcal{M}_j$ is parameterized with a $K_j$-vector of independent variables $\\Theta_{j}$, let's denote its distribution within model $\\mathcal{M}_j$ as:\n",
    "$$p\\left(\\left.\\Theta_{j}\\right|\\mathcal{M}_{j}\\right)$$\n",
    "- By law of total expectation:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "p\\left(\\left.\\mathbf{y}\\right|\\mathcal{M}_{j}\\right) & = & \\mathbb{E}_{\\left.\\Theta_{j}\\right|\\mathcal{M}_{j}}\\left[p\\left(\\left.\\mathbf{y}\\right|\\mathcal{M}_{j},\\Theta_{j}\\right)\\right]\\\\\n",
    " & = & \\int_{\\mathbb{R}^{K_{j}}}p\\left(\\left.\\mathbf{y}\\right|\\mathcal{M}_{j},\\Theta_{j}\\right)p\\left(\\left.\\Theta_{j}\\right|\\mathcal{M}_{j}\\right)\\mbox{d}\\Theta_{j}\\\\\n",
    " & = & \\int_{\\mathbb{R}^{K_{j}}}\\mathcal{L}_{\\mathcal{M}_{j}}\\left(\\left.\\Theta_{j}\\right|\\mathbf{y}\\right)p\\left(\\left.\\Theta_{j}\\right|\\mathcal{M}_{j}\\right)\\mbox{d}\\Theta_{j}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Bayesian information criterion</center>\n",
    "- Assuming most uninformative prior distribution of the parameters, Schwarz (1978) proved that\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\ln\\int_{\\mathbb{R}^{K}}\\mathcal{L}_{\\mathcal{M}_{j}}\\left(\\left.\\Theta_{j}\\right|\\mathbf{y}\\right)p\\left(\\left.\\Theta_{j}\\right|\\mathcal{M}_{j}\\right)\\mbox{d}\\Theta_{j} & \\sim & \\max_{\\Theta_{j}}\\left\\{ \\mathcal{L}_{\\mathcal{M}_{j}}\\left(\\left.\\Theta_{j}\\right|\\mathbf{y}\\right)\\right\\} -\\frac{K_{j}}{2}\\ln N\\\\\n",
    "\\mbox{as }N & \\rightarrow & \\infty\n",
    "\\end{eqnarray*}\n",
    "- Laplace's method for integral approximate:\n",
    "<center>An integral of a function \"peaky\" enough, is mostly determined by its peak value</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Bayesian information criterion</center>\n",
    "- With this approximation the posterior probability for model $\\mathcal{M}_j$ can be approxmiated by\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\ln P\\left(\\left.\\mathcal{M}_{j}\\right|\\mathbf{y}\\right) & \\sim & \\max_{\\Theta_{j}}\\left\\{ \\mathcal{L}_{\\mathcal{M}_{j}}\\left(\\left.\\Theta_{j}\\right|\\mathbf{y}\\right)\\right\\} -\\frac{K_{j}}{2}\\ln N+C\\\\\n",
    "\\mbox{as }N & \\rightarrow & \\infty\n",
    "\\end{eqnarray*}\n",
    "- Denote the MLE of $\\Theta_{j}$ as $\\hat{\\Theta}_{j}$, then the BIC is defined as:\n",
    "$$\\mbox{BIC}=-2\\mathcal{L}_{\\mathcal{M}_{j}}\\left(\\left.\\hat{\\Theta}_{j}\\right|\\mathbf{y}\\right)+K_{j}\\ln N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Bayesian information criterion</center>\n",
    "- BIC can be used to calculate the estimated posterior probabilities of all the models:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "-\\frac{1}{2}\\mbox{BIC}_{j} & \\sim & \\ln P\\left(\\left.\\mathcal{M}_{j}\\right|\\mathbf{y}\\right)+C\\\\\n",
    "\\exp\\left(-\\frac{1}{2}\\mbox{BIC}_{j}\\right) & \\sim & C\\cdot P\\left(\\left.\\mathcal{M}_{j}\\right|\\mathbf{y}\\right)\\\\\n",
    "\\frac{\\exp\\left(-\\frac{1}{2}\\mbox{BIC}_{j}\\right)}{\\sum_{j=1}^{M}\\exp\\left(-\\frac{1}{2}\\mbox{BIC}_{j}\\right)} & \\sim & P\\left(\\left.\\mathcal{M}_{j}\\right|\\mathbf{y}\\right)\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Summary of BIC</center>\n",
    "- With flat prior for the model and parameter distributions, BIC estimates the posterior model distribution by a set of weights\n",
    "- For model selection, the model with largest posterior is selected\n",
    "- BIC doesn't assume the existence of a single \"True\" model. Instead, it assumes the \"True\" model itself is random and tries to estimate its distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Summary of different model selection methods</center>\n",
    "- The purpose of model selection is to find a proper constraint on the parameter space to avoid over-fitting\n",
    "- The selection is based on several criteria including, test error, information loss and model posterior probability\n",
    "- Mallow's $C_p$, AIC, BIC are free, however the number of parameter are not usually easy to compute\n",
    "- Cross validation and Bootstrap are expensive and much more flexible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Thanks</center>\n",
    "References\n",
    "\n",
    "- T. Hastie, J. H. Friedman, and R. Tibshirani, The Elements of Statistical Learning. 2nd ed. 2009, Springer\n",
    "- X. Yan, and X. Su, Linear Regression Analysis: Theory and Computing 1st ed. 2009, World Scientific Publishing Company\n",
    "- T. M. Cover and J. A. Thomas, Elements of Information theory. 2nd ed. 2006, JOHN WILEY & SONS, INC.\n",
    "- Burnham and Anderson, Multimodel Inference Understanding AIC and BIC in Model Selection. 2004 SOCIOLOGICAL METHODS & RESEARCH, Vol. 33, No. 2, 261-304\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- enlarge the font size to suit slide show -->\n",
       "<style>\n",
       "    div.output_result {\n",
       "    font-size:150%;\n",
       "    line-height:150%;\n",
       "    }\n",
       "    \n",
       "    div.output_stdout { \n",
       "    font-size:150%;\n",
       "    line-height:150%;\n",
       "    }\n",
       "\n",
       "    div.prompt { /* hidden by the js block at the end */\n",
       "    font-size:100%;\n",
       "    line-height:100%;\n",
       "    }\n",
       "\n",
       "    div.CodeMirror { /* Code cell */\n",
       "    font-size:150%;\n",
       "    line-height:150%;\n",
       "    }\n",
       "    \n",
       "    div.text_cell_render { /* Markdown cell */\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size:150%;\n",
       "    line-height:150%;\n",
       "    }\n",
       "</style>\n",
       "\n",
       "<!-- hide elements on the left to center the slides -->\n",
       "<script>\n",
       "    $(document).ready(function(){\n",
       "        $('div.prompt').hide();\n",
       "        $('div.back-to-top').hide();\n",
       "        $('nav#menubar').hide();\n",
       "        $('.breadcrumb').hide();\n",
       "        $('.hidden-print').hide();\n",
       "    });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename='../slides.html')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:slides]",
   "language": "python",
   "name": "conda-env-slides-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "05a8dbeb-d54c-4e49-aafc-68b7e45719fd": {
     "id": "05a8dbeb-d54c-4e49-aafc-68b7e45719fd",
     "prev": "b9c9fcc5-51b0-4da4-8416-f11c87a438c5",
     "regions": {
      "746ce7f4-633e-40ed-9961-559d5ab3bd47": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "cd6f0a2a-2288-4cf2-a060-7f3e818a3a66",
        "part": "whole"
       },
       "id": "746ce7f4-633e-40ed-9961-559d5ab3bd47"
      }
     }
    },
    "2388de70-ea9e-414d-b2cc-190a1e7b207a": {
     "id": "2388de70-ea9e-414d-b2cc-190a1e7b207a",
     "prev": "b0c6975e-4388-4e12-b61c-8d3bcee65fed",
     "regions": {
      "bef4757c-eb56-4cf3-a9c9-c05105c9c739": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ef9df2b8-dd9c-49ba-b489-f7c0cef59906",
        "part": "whole"
       },
       "id": "bef4757c-eb56-4cf3-a9c9-c05105c9c739"
      }
     }
    },
    "43fc8132-b51d-4250-ab14-f1a5d2d5c36c": {
     "id": "43fc8132-b51d-4250-ab14-f1a5d2d5c36c",
     "prev": "2388de70-ea9e-414d-b2cc-190a1e7b207a",
     "regions": {
      "20f99b9a-644f-48ee-993c-8db67a607b67": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "cd6f0a2a-2288-4cf2-a060-7f3e818a3a66",
        "part": "whole"
       },
       "id": "20f99b9a-644f-48ee-993c-8db67a607b67"
      }
     }
    },
    "4bbf284f-80a5-40b7-b5fd-bcaec92e3afb": {
     "id": "4bbf284f-80a5-40b7-b5fd-bcaec92e3afb",
     "prev": "50eef625-1642-40a3-86a6-910695eb4410",
     "regions": {
      "3ef03aea-6e04-40a6-93ce-8817e73ac289": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "cd6f0a2a-2288-4cf2-a060-7f3e818a3a66",
        "part": "whole"
       },
       "id": "3ef03aea-6e04-40a6-93ce-8817e73ac289"
      }
     }
    },
    "50eef625-1642-40a3-86a6-910695eb4410": {
     "id": "50eef625-1642-40a3-86a6-910695eb4410",
     "prev": "43fc8132-b51d-4250-ab14-f1a5d2d5c36c",
     "regions": {
      "b434ef4e-3f8c-4d4e-a275-5c0c2d398017": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "cd6f0a2a-2288-4cf2-a060-7f3e818a3a66",
        "part": "whole"
       },
       "id": "b434ef4e-3f8c-4d4e-a275-5c0c2d398017"
      }
     }
    },
    "57fb394f-5e64-48e7-92a5-3208563cf1c6": {
     "id": "57fb394f-5e64-48e7-92a5-3208563cf1c6",
     "prev": "6c97ab4a-63cf-4509-9b70-5742e5df8d3b",
     "regions": {
      "3a6522b3-be6b-4d37-b920-fc5f68c0db11": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "cd6f0a2a-2288-4cf2-a060-7f3e818a3a66",
        "part": "whole"
       },
       "id": "3a6522b3-be6b-4d37-b920-fc5f68c0db11"
      }
     }
    },
    "6c97ab4a-63cf-4509-9b70-5742e5df8d3b": {
     "id": "6c97ab4a-63cf-4509-9b70-5742e5df8d3b",
     "prev": "4bbf284f-80a5-40b7-b5fd-bcaec92e3afb",
     "regions": {
      "cb9d2e9a-a5c8-4abb-b62f-624acaad7b75": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "cd6f0a2a-2288-4cf2-a060-7f3e818a3a66",
        "part": "whole"
       },
       "id": "cb9d2e9a-a5c8-4abb-b62f-624acaad7b75"
      }
     }
    },
    "b0c6975e-4388-4e12-b61c-8d3bcee65fed": {
     "id": "b0c6975e-4388-4e12-b61c-8d3bcee65fed",
     "prev": "f9c1903a-c098-48bf-a8c7-b324a34d9af4",
     "regions": {
      "f1ebc62d-e674-47d7-9f1b-b40dab0fab4a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3e13915c-3e50-4d97-92af-dc493d409193",
        "part": "whole"
       },
       "id": "f1ebc62d-e674-47d7-9f1b-b40dab0fab4a"
      }
     }
    },
    "b9c9fcc5-51b0-4da4-8416-f11c87a438c5": {
     "id": "b9c9fcc5-51b0-4da4-8416-f11c87a438c5",
     "prev": "57fb394f-5e64-48e7-92a5-3208563cf1c6",
     "regions": {
      "f0317884-d843-42fb-bb7e-c2b96698812a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "cd6f0a2a-2288-4cf2-a060-7f3e818a3a66",
        "part": "whole"
       },
       "id": "f0317884-d843-42fb-bb7e-c2b96698812a"
      }
     }
    },
    "f9c1903a-c098-48bf-a8c7-b324a34d9af4": {
     "id": "f9c1903a-c098-48bf-a8c7-b324a34d9af4",
     "prev": null,
     "regions": {
      "5a039c08-01d5-4a10-b2ed-04bb7679890f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f4e8d878-6137-4252-898b-dbfa915dec9b",
        "part": "whole"
       },
       "id": "5a039c08-01d5-4a10-b2ed-04bb7679890f"
      }
     }
    }
   },
   "themes": {
    "default": "03b5650f-0d59-4991-a7b0-447a9541a40c",
    "theme": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
